import numpy as np
from linear_regression.gradient_descent import stochastic_gradient_descent

class LinearRegression:
    def __init__(self, X: np.ndarray, y:np.ndarray, dtype: str = "float64"):

        # check X, y dimensions
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"The variables 'X' and 'y' must have the same number of rows. While 'X' has {X.shape[0]} rows, and 'y' {y.shape[0]}.")
        
        # instantiate the attributes
        self._dtype = dtype
        self._X = np.insert(X, 0, 1, axis=1).astype(dtype=self._dtype) 
        self.y = y.astype(dtype=self._dtype) 
        self._fit_method_called = None

    def fit(self, learning_rate: float = 0.01, batch_size: int = None, n_iter: int = 100, random_seed: int = None) -> None:
        """
        Uses a gradient descent algorithm to fit the parameters of a linear regression model from a training set.
        If you want to use a stochastic gradient descent define a strictly positive 'batch_size'.

        Args:
            learning_rate: learning rate for the gradient descent
            batch_size: batch size for the stochastic gradient descent.
            n_iter: number of iteration of the (stochastic) gradient descent algorithm
            random_seed: define the random seed for the numpy random number generator if you want to be able to reproduce the experiment.

        Returns:
            None
        """
        # initialize the X matrix and the y vector and the parameters vector

         # initialize the parameter to train
        self.theta = np.ones(shape=(self._X.shape[1], 1), dtype=self._dtype) 

        # compute the (stochastic) gradient descent
        self.theta, self._theta_training_history = stochastic_gradient_descent(
                                vector=self.theta,
                                learning_rate=learning_rate,
                                gradient=LinearRegression.mse_gradient,
                                X=self._X, 
                                y=self.y,
                                batch_size=batch_size,
                                n_iter=n_iter,
                                random_seed=random_seed
                                )
        
        # set the attribute that check if the method has been successfully called to True
        self._fit_method_called = True

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Returns the image of the fitted linear regression model.

        Args:
            X: the features for which I want to predict the target.

        Returns:
            The prediction obtained with the linear regression model obtained.
        """
        # check if the model has been fit
        if not self._fit_method_called:
            raise RuntimeError("The method predict can not be called without calling the fit method sucessfully first.")
            
        X = np.insert(X, 0, 1, axis=1)
        return X @ self.theta

    def mse_training_history(self) -> list:
        """
        This method compute the mean square error (mse) function on each step of the gradient descent algorithm.

        Returns:
            A list with the result of the results of the mse function computed on the vectors generated by the gradient descent algorithm.
        """

        if not self._fit_method_called:
            raise RuntimeError("The method predict can not be called without calling the fit method sucessfully first.")

        mse_training_history=[LinearRegression.mse(theta, self._X, self.y) for theta in self._theta_training_history]
        return mse_training_history


            
    def analytical_estimation(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """
        Compute the parameters of the linear regression model analytically.

        Args:
            X: the matrix containing the features of the training set.
            y: the vector containing the targets of the training set.
        
        Returns:
            The parameters of the linear regression model computed analytically.
        """
    
        X = np.insert(X, 0, 1, axis=1)

        XTX = X.T @ X
        det = np.linalg.det(XTX)

        if det == 0:
            raise ValueError("The determinant of the matrix np.dot(X.T,X) is 0, this algorithm can not minimize the cost function.")

        XTX_inv = np.linalg.inv(XTX)
        XTy = X.T @ y
        return XTX_inv @ XTy

    @staticmethod
    def mse(theta: np.ndarray, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """
        The MSE (means square error function) is the cost function used for this linear regression. 
        It is a function from a vector space, which has dimension equal to the number of elements of the training set, to the real line.

        Args:
            theta: the argument of the MSE function.
            X: The matrix generated by the features of the training set, it is used to define the MSE.
            y: the vector generated by the targets of the training set, it is used to define the MSE.

        Returns:
        A np.array that contains a scalar.
        """
        errors = y - X @ theta
        return np.divide(errors.T @ errors, 2 * X.shape[0])[0][0]

    @staticmethod
    def mse_gradient(theta: np.ndarray, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """
        Returns the gradient of the MSE (mean square error) cost function.
        This is a linear endofunction whose domain is and codomain are the domain of the mean square error function. 


        Args:
            theta: the argument of the MSE gradient.
            X: the features of the training set used to generate the MSE cost function.
            y: the targets of the training set used to generate the MSE cost function.
        
        Returns:
            A column vector that is the image of the MSE cost function gradient.
        """
        return np.divide(X.T @ (X @ theta - y), X.shape[0])
